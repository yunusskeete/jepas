{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from decord import VideoReader, cpu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from model import VJEPA\n",
    "from finetune_VJEPA import VJEPA_FT\n",
    "from finetune_TRJEPA import TRJEPA_FT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Video and create a stacked image tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAMES_PER_CLIP: int = 8\n",
    "VIDEO_PATH: str = (\n",
    "    \"E:/ahmad/kinetics-dataset/extrasmall/val/part_0/-_3E3GBXAUc_000010_000020.mp4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_tensor_as_frames(tensor):\n",
    "    \"\"\"\n",
    "    Render a tensor of shape [1, channels, frames, height, width] as frames in the notebook.\n",
    "\n",
    "    Args:\n",
    "        tensor: Input tensor with shape [1, channels, frames, height, width].\n",
    "    \"\"\"\n",
    "    # Remove the batch dimension\n",
    "    tensor = tensor.squeeze(0)  # Shape: [channels, frames, height, width]\n",
    "    tensor = tensor.cpu()\n",
    "\n",
    "    # Display each frame\n",
    "    for i in range(tensor.shape[1]):\n",
    "        frame = tensor[:, i, :, :]\n",
    "        # Normalize the frame\n",
    "        original_frame = frame - frame.min()  # Shift the minimum value to 0\n",
    "        original_frame = original_frame / original_frame.max()  # Normalize to [0, 1]\n",
    "        original_frame = original_frame * 255  # Scale to [0, 255]\n",
    "        original_frame_image = original_frame.permute(1, 2, 0).byte()\n",
    "        original_pil_image = Image.fromarray(\n",
    "            original_frame_image.numpy()\n",
    "        )  # Convert to PIL Image\n",
    "\n",
    "        # Plot the frame\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(original_pil_image)\n",
    "        plt.title(f\"Frame {i+1}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video_with_decord(video_path, transform, max_frames=None):\n",
    "    vr = VideoReader(video_path, ctx=cpu(0))\n",
    "\n",
    "    # Extract frames as a list of numpy arrays\n",
    "    frames = [vr[i].asnumpy() for i in range(len(vr))[:max_frames]]\n",
    "\n",
    "    # Convert the list of numpy arrays to a single numpy array\n",
    "    frames = np.stack(frames)  # Shape: [frames, height, width, channels]\n",
    "\n",
    "    # Convert to PyTorch tensor, reorder dimensions, and normalize\n",
    "    frames = (\n",
    "        torch.tensor(frames).permute(0, 3, 1, 2).float() / 255.0\n",
    "    )  # [frames, channels, height, width]\n",
    "\n",
    "    # Apply transform to each frame\n",
    "    transformed_frames = torch.stack(\n",
    "        [transform(frame) for frame in frames]\n",
    "    )  # [frames, channels, height, width]\n",
    "\n",
    "    # Add batch dimension and reorder to [batch_size, channels, frames, height, width]\n",
    "    return transformed_frames.unsqueeze(0).permute(0, 2, 1, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "def make_transforms(\n",
    "    random_horizontal_flip,\n",
    "    random_resize_aspect_ratio,\n",
    "    random_resize_scale,\n",
    "    reprob,\n",
    "    auto_augment,\n",
    "    motion_shift,\n",
    "    crop_size,\n",
    "):\n",
    "    transforms = [\n",
    "        T.Resize((crop_size, crop_size)),  # Resize to (crop_size, crop_size)\n",
    "        T.RandomHorizontalFlip(\n",
    "            p=0.5 if random_horizontal_flip else 0.0\n",
    "        ),  # Horizontal flip\n",
    "        T.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "        ),  # Normalization\n",
    "    ]\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = make_transforms(\n",
    "    random_horizontal_flip=True,\n",
    "    random_resize_aspect_ratio=[3 / 4, 4 / 3],\n",
    "    random_resize_scale=[0.3, 1.0],\n",
    "    reprob=0.0,\n",
    "    auto_augment=False,\n",
    "    motion_shift=False,\n",
    "    crop_size=224,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load video and apply transforms\n",
    "original_video = load_video_with_decord(\n",
    "    video_path=VIDEO_PATH, transform=transform, max_frames=FRAMES_PER_CLIP\n",
    ")\n",
    "print(f\"Tensor shape: {original_video.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_tensor_as_frames(original_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Visualise Stacked Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Get first frame and stack\n",
    "##############################\n",
    "x = original_video[:, :, 0:1, :, :]  # Get first frame and stack\n",
    "stacked_img = x.repeat(1, 1, FRAMES_PER_CLIP, 1, 1)\n",
    "stacked_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_tensor_as_frames(stacked_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pretrain TRJEPA and Finetune TRJEPA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Load Pretrained TRJEPA model\n",
    "##############################\n",
    "# model = VJEPA.load_from_checkpoint(\n",
    "#     \"D:/MDX/Thesis/new-jepa/jepa/lightning_logs/v-jepa/pretrain/static_scene/version_6/checkpoints/epoch=2-step=90474.ckpt\"\n",
    "# )\n",
    "model = VJEPA(lr=1e-3, num_frames=FRAMES_PER_CLIP, testing_purposes_only=True)\n",
    "\n",
    "finetune_vjepa_path: Optional[str] = None\n",
    "finetune_vjepa_model: Optional[VJEPA_FT] = None\n",
    "\n",
    "if finetune_vjepa_path is not None:\n",
    "    finetune_vjepa_model = VJEPA_FT.load_from_checkpoint(finetune_vjepa_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Load Finetuned TRJEPA model\n",
    "##############################\n",
    "finetune_model = TRJEPA_FT(\n",
    "    vjepa_model=model,\n",
    "    finetune_vjepa_model=finetune_vjepa_model,\n",
    "    frame_count=FRAMES_PER_CLIP,\n",
    ")\n",
    "\n",
    "finetune_trjepa_path: Optional[str] = None\n",
    "if finetune_trjepa_path is not None:\n",
    "    finetune_model = TRJEPA_FT.load_from_checkpoint(\n",
    "        finetune_trjepa_path,\n",
    "        vjepa_model=model,\n",
    "        finetune_vjepa_model=finetune_vjepa_model,\n",
    "        frame_count=dataset.frames_per_clip,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pass Stacked image tensor through model and visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = finetune_model(x=stacked_img, random_t=0)\n",
    "render_tensor_as_frames(tensor=result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caluclate Loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = finetune_model.criterion(result, original_video)  # calculate loss\n",
    "accuracy = (result.argmax(dim=1) == original_video.argmax(dim=1)).float().mean()\n",
    "print(f\"Loss={loss} Accuracy={accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
